{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9740a075",
   "metadata": {},
   "source": [
    "# ДЗ1 Анна Головина группа БКЛ213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27b0e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps, loads\n",
    "import jsonlines\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d6894",
   "metadata": {},
   "source": [
    "### 2 пункт\n",
    "\n",
    "Поскольку в тетрадке с семинара при лемматизации пунктуация сохранялась, по аналогии здесь пунктуация тоже сохранена. Исходный файл $-$ *pokushenie.txt*. Лемматизированный текст с сохраненной пунктуацией записывается в файл *pokushenie_lemmas.txt*. Решение совровождается таймером."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6831c605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open(\"pokushenie.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    text = file.read()\n",
    "    m = Mystem()\n",
    "    lemmas = m.lemmatize(text)\n",
    "    with open(\"pokushenie_lemmas.txt\", \"w\", encoding=\"utf8\") as new_file:\n",
    "        new_file.write(\"\".join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae333f",
   "metadata": {},
   "source": [
    "### 3 пункт\n",
    "\n",
    "Здесь уже исключаем пунктуацию, чтобы избежать ошибок при определении части речи. Убирала только то, что входит в строчку punctuation, поскольку она позволяет игнорировать основную массу знаков препинания, а какие-то нестандартные символы (коих меньшинство) можно вычистить из файла при необходимости уже вручную. Для токенизации берется текст из переменной, в которую уже загрузили текст файла. Запись осуществляется циклом по каждому токену, параметр ensure_ascii имеет значение False, полскольку в тексте кириллица. Чтобы не перегружать код условиями, оставила цифры (указание на годы), но в итоговом файле их частеречная принадлежность не определена (ключ *\"pos\"* имеет значение *\"null\"*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ad7299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "list_of_tokens = word_tokenize(text)\n",
    "with jsonlines.open(\"pokushenie_gram.jsonl\", mode=\"a\") as writer:\n",
    "    for token in list_of_tokens:\n",
    "        if token not in punctuation:\n",
    "            line = {}\n",
    "            info = morph.parse(token)[0]\n",
    "            line[\"lemma\"] = info.normal_form\n",
    "            line[\"word\"] = info.word\n",
    "            line[\"pos\"] = info.tag.POS\n",
    "            writer.write(dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c582468",
   "metadata": {},
   "source": [
    "### 4 пункт\n",
    "\n",
    "Для получения грамматической информации открываем файл на чтение и преобразуем всю полученную информацию в список. По длине списка получаем общее кол-во словоформ, т.е. знаменатель дроби при вычислении доли. Далее создаем словарь, в котором ключи $-$ названия частей речи, а значения $-$ количество словоформ, относящихся к этой части речи (None игнорируем). Я, к сожалению, не смогла придумать ничего оптимальнее, чем для получения доли от общего количества слов второй раз проходиться по ключам словаря (потому что за первый проход, т.е. собственно составление словаря значения меняются). Таким образом, получаем дроби и выводим словарь с частотностью частей речи на печать (проходясь по ключам, потому что так понятнее и симпатичнее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "935e3489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частотность NOUN равна 0.25586282121202397\n",
      "Частотность PREP равна 0.09752654711109686\n",
      "Частотность VERB равна 0.13092297327644284\n",
      "Частотность CONJ равна 0.08317025440313111\n",
      "Частотность ADJF равна 0.12126656186840333\n",
      "Частотность PRCL равна 0.06108241634852908\n",
      "Частотность NPRO равна 0.07468480318244522\n",
      "Частотность ADVB равна 0.06287895800583876\n",
      "Частотность ADJS равна 0.014067562798755253\n",
      "Частотность INFN равна 0.03315581790767059\n",
      "Частотность GRND равна 0.0036251644156427448\n",
      "Частотность PRTS равна 0.0039780565269000034\n",
      "Частотность PRTF равна 0.017099226845465335\n",
      "Частотность PRED равна 0.0055339899265342786\n",
      "Частотность COMP равна 0.005229219466812101\n",
      "Частотность INTJ равна 0.0016682172532161304\n",
      "Частотность NUMR равна 0.003320393955920567\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"pokushenie_gram.jsonl\", \"r\") as f:\n",
    "    data = [token for token in f]\n",
    "    quantity_gen = len(data)\n",
    "    quantity_pos = {}\n",
    "    for item in data:\n",
    "        word = loads(item)\n",
    "        if word[\"pos\"] != None:\n",
    "            quantity_pos[word[\"pos\"]] = quantity_pos.get(word[\"pos\"], 0) + 1\n",
    "    for pos, quantity_ind in quantity_pos.items():\n",
    "        print(f\"Частотность {pos} равна {quantity_ind/quantity_gen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e0055",
   "metadata": {},
   "source": [
    "Функция здесь берет в качестве аргумента частеречный ярлык. Создается словарь, который считает частотность конкретных лемм при условии соответствия части речи ярлыку. Далее словарь сортируется по убыванию частотности. Функция возвращает список 20 самых частотных лемм-ключей. Функция далее реализуется для глаголов и наречий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f260170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 глаголов:\n",
      "быть\n",
      "мочь\n",
      "стать\n",
      "хотеть\n",
      "знать\n",
      "сказать\n",
      "ждать\n",
      "любить\n",
      "видеть\n",
      "жить\n",
      "говорить\n",
      "казаться\n",
      "идти\n",
      "савть\n",
      "считать\n",
      "молчать\n",
      "иметь\n",
      "прийтись\n",
      "прийти\n",
      "стоить\n",
      "Топ-20 наречий:\n",
      "только\n",
      "ещё\n",
      "уже\n",
      "сейчас\n",
      "тогда\n",
      "теперь\n",
      "уж\n",
      "ничего\n",
      "где\n",
      "потому\n",
      "тут\n",
      "столь\n",
      "всегда\n",
      "там\n",
      "никогда\n",
      "много\n",
      "что-то\n",
      "зачем\n",
      "снова\n",
      "сразу\n"
     ]
    }
   ],
   "source": [
    "def top_list(label):\n",
    "    pos_d = {}\n",
    "    for item in data:\n",
    "        word = loads(item)\n",
    "        if word[\"pos\"] == label:\n",
    "            pos_d[word[\"lemma\"]] = pos_d.get(word[\"lemma\"], 0) + 1\n",
    "        else:\n",
    "            continue\n",
    "    pos_sorted = {i: pos_d[i] for i in sorted(pos_d, key=pos_d.get, reverse=True)}\n",
    "    res = list(pos_sorted.keys())[:20]\n",
    "    return res\n",
    "        \n",
    "print(\"Топ-20 глаголов:\")\n",
    "print(*top_list(\"VERB\"), sep=\"\\n\")\n",
    "print(\"Топ-20 наречий:\")\n",
    "print(*top_list(\"ADVB\"), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efc515",
   "metadata": {},
   "source": [
    "### 5 пункт\n",
    "\n",
    "Перед применением функции уже имеющийся список токенов обновляется (убираются все токены, содержащие небуквенные символы).\n",
    "\n",
    "Функция имеет в качестве аргумента список n-грамм. Создается пустой словарь, который заполняется n-граммами и их количеством в тексте. Далее словарь сортируется по убыванию значений (сортировка, как в предыдущих ячейках через get не сработала, поэтому была применена сортировка через лямбда-функции). Функция возвращает список 25 самых частотных n-грамм.\n",
    "\n",
    "Применяем функцию к спискам биграмм и триграмм на основе обновленного списка токенов.\n",
    "\n",
    "В моем случае получились именно такие (чаще со служебными частями речи) биграммы и триграммы, поскольку я не вычищала стоп-слова. Второй самой частотной группой биграмм являются имена собственные (поскольку героев часто называют по имени и отчеству). Среди триграмм второй самой частотной группой являются услоявшиеся выражения наподобие *едва ли не* и *изо всех сил*, поскольку они в целом частотны в речевом узусе носителей русского языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2047f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-25 биграмм:\n",
      "Георгий Петрович\n",
      "и не\n",
      "ничего не\n",
      "еще не\n",
      "Толя Зыбков\n",
      "не только\n",
      "уже не\n",
      "никогда не\n",
      "у меня\n",
      "Иван Трофимович\n",
      "на меня\n",
      "с ним\n",
      "Миша Дедушка\n",
      "но не\n",
      "не мог\n",
      "тех кто\n",
      "я не\n",
      "И я\n",
      "вместе с\n",
      "никак не\n",
      "в нем\n",
      "а потому\n",
      "и в\n",
      "в том\n",
      "Ирина Михайловна\n",
      "Топ-25 триграмм:\n",
      "едва ли не\n",
      "из тех кто\n",
      "на этот раз\n",
      "себя Сыном Человеческим\n",
      "вместе с ним\n",
      "так и не\n",
      "ни у кого\n",
      "о том что\n",
      "в свое время\n",
      "все ли равно\n",
      "у кого не\n",
      "друг с другом\n",
      "не из тех\n",
      "До сих пор\n",
      "в его сторону\n",
      "с ним и\n",
      "И что же\n",
      "направо слева направо\n",
      "тем не менее\n",
      "в стороне от\n",
      "друг к другу\n",
      "потому только что\n",
      "изо всех сил\n",
      "то же самое\n",
      "Георгий Петрович на\n"
     ]
    }
   ],
   "source": [
    "def frequency_count(ngrams):\n",
    "    freq_dict = {}\n",
    "    for ngram in ngrams:\n",
    "        freq_dict[ngram] = freq_dict.get(ngram, 0) + 1\n",
    "    ngram_sorted = list(sorted(freq_dict.items(), reverse = True, key=lambda x: x[1]))[:25]\n",
    "    res = [' '.join(ng[0]) for ng in ngram_sorted]\n",
    "    return res\n",
    "\n",
    "new_tokens = [token for token in list_of_tokens if token.isalpha() == True]\n",
    "\n",
    "print(\"Топ-25 биграмм:\")\n",
    "print(*frequency_count(nltk.bigrams(new_tokens)), sep = \"\\n\")\n",
    "print(\"Топ-25 триграмм:\")\n",
    "print(*frequency_count(nltk.trigrams(new_tokens)), sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870765f",
   "metadata": {},
   "source": [
    "### 6 пункт\n",
    "\n",
    "Я пыталась поменять конкретные теги, не затрагивая прочую гармматическую информацию о словах, кроме того, что требуется изменить. Почему-то здесь достаточно часто слетают падежи, а в каком-никаком тексте более двух предложений отслеживание еще и падежных тегов сделает код совсем громоздким.\n",
    "\n",
    "Местоимения не меняла, обрабатывала только существительные, прилагательные и глаголы, так что вышел не очень красивый и грамматичный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db66d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перед вечером морях галилейские замирает: волн отбегают от оглаженных валунов, обессиленно лежат на гальке, лишь бесшумно вздыхают. По берега красные, прокаленные скала рвутся из сочных зелени, над ними сведенные судорогами сосенок обнимаются с жирным олеандрами. Вспыхивают весло на солнце, гонят неуклюжих рыбачьих барку. пути недалек, в ближайшие городишки Вифсаиду, он уже виден впереди - по склонам лепятся друзья над друзьями плоский крыш и клочковатый садики.мы В барках десятки мужчин, рыбака из Капернаума.\n"
     ]
    }
   ],
   "source": [
    "original = \"\"\"Перед вечером море Галилейское замирает: волны отбегают\n",
    "от оглаженных валунов, обессиленно лежат на гальке, лишь бесшумно\n",
    "вздыхают. По берегу красные, прокаленные скалы рвутся из\n",
    "сочной зелени, над ними сведенные судорогой сосенки обнимаются\n",
    "с жирными олеандрами. Вспыхивают весла на солнце, гонят неуклюжую\n",
    "рыбачью барку. Путь недалек, в ближайший городишко Вифсаиду, он\n",
    "уже виден впереди - по склону лепятся друг над другом плоские крыши\n",
    "и клочковатые садики.м В барке десяток мужчин, рыбаков из Капернаума.\"\"\"\n",
    "rewritten = []\n",
    "tokens = original.split()\n",
    "morph = MorphAnalyzer()\n",
    "for token in tokens:\n",
    "    if (\n",
    "        morph.parse(token)[0].tag.POS == \"NOUN\"\n",
    "        or morph.parse(token)[0].tag.POS == \"ADJF\"\n",
    "    ) and morph.parse(token)[0].tag.number == \"plur\":\n",
    "        rewritten.append(morph.parse(token)[0].inflect({\"sing\"}).word)\n",
    "    elif (\n",
    "        morph.parse(token)[0].tag.POS == \"NOUN\"\n",
    "        or morph.parse(token)[0].tag.POS == \"ADJF\"\n",
    "    ) and morph.parse(token)[0].tag.number == \"sing\":\n",
    "        rewritten.append(morph.parse(token)[0].inflect({\"plur\"}).word)\n",
    "    elif (\n",
    "        (\n",
    "            morph.parse(token)[0].tag.POS == \"VERB\"\n",
    "            or morph.parse(token)[0].tag.POS == \"INFN\"\n",
    "        )\n",
    "        and morph.parse(token)[0].tag.tense == \"past\"\n",
    "        and morph.parse(token)[0].tag.number == \"sing\"\n",
    "    ):\n",
    "        rewritten.append(morph.parse(token)[0].inflect({\"3per\", \"plur\", \"pres\"}).word)\n",
    "    elif (\n",
    "        (\n",
    "            morph.parse(token)[0].tag.POS == \"VERB\"\n",
    "            or morph.parse(token)[0].tag.POS == \"INFN\"\n",
    "        )\n",
    "        and morph.parse(token)[0].tag.tense == \"past\"\n",
    "        and morph.parse(token)[0].tag.number == \"plur\"\n",
    "    ):\n",
    "        rewritten.append(morph.parse(token)[0].inflect({\"3per\", \"sing\", \"pres\"}).word)\n",
    "    else:\n",
    "        rewritten.append(token)\n",
    "print(\" \".join(rewritten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d4798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
